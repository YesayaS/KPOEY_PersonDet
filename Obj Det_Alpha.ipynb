{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 144293,
     "status": "ok",
     "timestamp": 1603009575145,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "Gb37zJgyUy7k",
    "outputId": "cc10749c-9a7d-4fbb-dca4-67e9a601ecfe"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import pathlib\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 164362,
     "status": "ok",
     "timestamp": 1603009595314,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "a6ZlblEdU3bj",
    "outputId": "5361e2d4-894b-47f2-f196-7c8b6eda8232"
   },
   "source": [
    "#!pip install -U --pre tensorflow==\"2.*\"\n",
    "!pip install tf_slim\n",
    "!pip install pycocotools\n",
    "!pip install keyboard\n",
    "!apt-get install protobuf-compiler python-pil python-lxml python-tk\n",
    "!pip install Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BujGJ6ynVHjS"
   },
   "outputs": [],
   "source": [
    "# CLONE DE GIT\n",
    "# try:\n",
    "#   if \"drive/My\\ Drive/models\" in pathlib.Path.cwd().parts:\n",
    "#     while \"drive/My\\ Drive/models\" in pathlib.Path.cwd().parts:\n",
    "#       os.chdir('/content/drive/My Drive')\n",
    "#   elif not pathlib.Path('drive/My\\ Drive/models').exists():\n",
    "#     os.chdir('/content/drive/My Drive')\n",
    "#     !git clone --depth 1 https://github.com/tensorflow/models\n",
    "# except:\n",
    "#   None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 15293,
     "status": "ok",
     "timestamp": 1603009595316,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "x0GZaooNVPZ9",
    "outputId": "cd7012e1-e64b-4068-817e-44fb80200158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeset\\tf_objdetection\\models\\research\n"
     ]
    }
   ],
   "source": [
    "# compile proto files\n",
    "# make sure you are in BASE_DIR/research/ (see cell below)\n",
    "%cd C:/Users/yeset/tf_objdetection/models/research\n",
    "# !protoc object_detection/protos/*.proto --python_out=.\n",
    "# !cp object_detection/packages/tf2/setup.py .\n",
    "# !python -m pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 15796,
     "status": "ok",
     "timestamp": 1603009596594,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "g-XifOKSVRUI",
    "outputId": "5a822e2c-b65d-4050-c96e-1b72e0d2372b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeset\\tf_objdetection\\models\\research\n",
      "C:\\Users\\yeset\\tf_objdetection\\models\\research\\object_detection\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())\n",
    "os.chdir(\"C:/Users/yeset/tf_objdetection/models/research/object_detection\")\n",
    "print(os.getcwd())\n",
    "try:\n",
    "    DATA_DIR = os.path.join(os.getcwd(), 'data')\n",
    "    MODELS_DIR = os.path.join(DATA_DIR, 'models')\n",
    "    for dir in [DATA_DIR, MODELS_DIR]:\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "except:\n",
    "    print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "executionInfo": {
     "elapsed": 1107,
     "status": "ok",
     "timestamp": 1603009597704,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "d9tM2U5oVTGI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "# os.chdir('/research')\n",
    "\n",
    "# Download and extract model\n",
    "MODEL_DATE = '20200711'\n",
    "#'efficientdet_d2_coco17_tpu-32' #my primary model\n",
    "#'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8' #secondary model (worst)\n",
    "#'efficientdet_d3_coco17_tpu-32' #third model (should be finest)\n",
    "MODEL_NAME = 'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8'\n",
    "MODEL_TAR_FILENAME = MODEL_NAME + '.tar.gz'\n",
    "MODELS_DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/tf2/'\n",
    "MODEL_DOWNLOAD_LINK = MODELS_DOWNLOAD_BASE + MODEL_DATE + '/' + MODEL_TAR_FILENAME\n",
    "PATH_TO_MODEL_TAR = os.path.join(MODELS_DIR, MODEL_TAR_FILENAME)\n",
    "PATH_TO_CKPT = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'checkpoint/'))\n",
    "PATH_TO_CFG = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, 'pipeline.config'))\n",
    "if not os.path.exists(PATH_TO_CKPT):\n",
    "    print('Downloading model. This may take a while... ', end='')\n",
    "    urllib.request.urlretrieve(MODEL_DOWNLOAD_LINK, PATH_TO_MODEL_TAR)\n",
    "    tar_file = tarfile.open(PATH_TO_MODEL_TAR)\n",
    "    tar_file.extractall(MODELS_DIR)\n",
    "    tar_file.close()\n",
    "    os.remove(PATH_TO_MODEL_TAR)\n",
    "    print('Done')\n",
    "\n",
    "# Download labels file\n",
    "LABEL_FILENAME = 'mscoco_label_map.pbtxt'\n",
    "LABELS_DOWNLOAD_BASE = \\\n",
    "    'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/data/'\n",
    "PATH_TO_LABELS = os.path.join(MODELS_DIR, os.path.join(MODEL_NAME, LABEL_FILENAME))\n",
    "if not os.path.exists(PATH_TO_LABELS):\n",
    "    print('Downloading label file... ', end='')\n",
    "    urllib.request.urlretrieve(LABELS_DOWNLOAD_BASE + LABEL_FILENAME, PATH_TO_LABELS)\n",
    "    print('Done')# %%bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1603009597706,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "cBXeXmJjVWzV",
    "outputId": "38d276f1-ef58-4a15-f49d-1c8443279f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeset\\tf_objdetection\\models\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1603009597706,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "Mr5OHBZFVa-a",
    "outputId": "5a947941-63a1-466b-f45a-28b31a19445a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeset\\tf_objdetection\\models\\research\\object_detection\\data\\models\\centernet_resnet101_v1_fpn_512x512_coco17_tpu-8\\pipeline.config\n"
     ]
    }
   ],
   "source": [
    "print(PATH_TO_CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "executionInfo": {
     "elapsed": 53957,
     "status": "ok",
     "timestamp": 1603009650592,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "sS30S2jAVb6p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "os.chdir('C:/Users/yeset/tf_objdetection/models/research')\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'    # Suppress TensorFlow logging\n",
    "import tensorflow as tf\n",
    "from object_detection.utils import label_map_util\n",
    "from object_detection.utils import config_util\n",
    "from object_detection.utils import visualization_utils as viz_utils\n",
    "os.chdir('C:/Users/yeset/tf_objdetection/models')\n",
    "from object_detection.builders import model_builder\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')           # Suppress TensorFlow logging (2)\n",
    "\n",
    "# Enable GPU dynamic memory allocation\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "# Load pipeline config and build a detection model\n",
    "configs = config_util.get_configs_from_pipeline_file(PATH_TO_CFG)\n",
    "model_config = configs['model']\n",
    "detection_model = model_builder.build(model_config=model_config, is_training=False)\n",
    "\n",
    "# Restore checkpoint\n",
    "ckpt = tf.compat.v2.train.Checkpoint(model=detection_model)\n",
    "ckpt.restore(os.path.join(PATH_TO_CKPT, 'ckpt-0')).expect_partial()\n",
    "\n",
    "@tf.function\n",
    "def detect_fn(image):\n",
    "    \"\"\"Detect objects in image.\"\"\"\n",
    "\n",
    "    image, shapes = detection_model.preprocess(image)\n",
    "    prediction_dict = detection_model.predict(image, shapes)\n",
    "    detections = detection_model.postprocess(prediction_dict, shapes)\n",
    "\n",
    "    return detections, prediction_dict, tf.reshape(shapes, [-1])\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 54256,
     "status": "ok",
     "timestamp": 1603009650895,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "4SW3GzobVc_E"
   },
   "outputs": [],
   "source": [
    "category_index = label_map_util.create_category_index_from_labelmap(PATH_TO_LABELS,\n",
    "                                                                    use_display_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "executionInfo": {
     "elapsed": 54244,
     "status": "ok",
     "timestamp": 1603009650896,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "5jhOwPuDVeVS",
    "outputId": "634e136f-2df7-4748-c1da-6f35fa4816c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: {'id': 1, 'name': 'person'}, 2: {'id': 2, 'name': 'bicycle'}, 3: {'id': 3, 'name': 'car'}, 4: {'id': 4, 'name': 'motorcycle'}, 5: {'id': 5, 'name': 'airplane'}, 6: {'id': 6, 'name': 'bus'}, 7: {'id': 7, 'name': 'train'}, 8: {'id': 8, 'name': 'truck'}, 9: {'id': 9, 'name': 'boat'}, 10: {'id': 10, 'name': 'traffic light'}, 11: {'id': 11, 'name': 'fire hydrant'}, 13: {'id': 13, 'name': 'stop sign'}, 14: {'id': 14, 'name': 'parking meter'}, 15: {'id': 15, 'name': 'bench'}, 16: {'id': 16, 'name': 'bird'}, 17: {'id': 17, 'name': 'cat'}, 18: {'id': 18, 'name': 'dog'}, 19: {'id': 19, 'name': 'horse'}, 20: {'id': 20, 'name': 'sheep'}, 21: {'id': 21, 'name': 'cow'}, 22: {'id': 22, 'name': 'elephant'}, 23: {'id': 23, 'name': 'bear'}, 24: {'id': 24, 'name': 'zebra'}, 25: {'id': 25, 'name': 'giraffe'}, 27: {'id': 27, 'name': 'backpack'}, 28: {'id': 28, 'name': 'umbrella'}, 31: {'id': 31, 'name': 'handbag'}, 32: {'id': 32, 'name': 'tie'}, 33: {'id': 33, 'name': 'suitcase'}, 34: {'id': 34, 'name': 'frisbee'}, 35: {'id': 35, 'name': 'skis'}, 36: {'id': 36, 'name': 'snowboard'}, 37: {'id': 37, 'name': 'sports ball'}, 38: {'id': 38, 'name': 'kite'}, 39: {'id': 39, 'name': 'baseball bat'}, 40: {'id': 40, 'name': 'baseball glove'}, 41: {'id': 41, 'name': 'skateboard'}, 42: {'id': 42, 'name': 'surfboard'}, 43: {'id': 43, 'name': 'tennis racket'}, 44: {'id': 44, 'name': 'bottle'}, 46: {'id': 46, 'name': 'wine glass'}, 47: {'id': 47, 'name': 'cup'}, 48: {'id': 48, 'name': 'fork'}, 49: {'id': 49, 'name': 'knife'}, 50: {'id': 50, 'name': 'spoon'}, 51: {'id': 51, 'name': 'bowl'}, 52: {'id': 52, 'name': 'banana'}, 53: {'id': 53, 'name': 'apple'}, 54: {'id': 54, 'name': 'sandwich'}, 55: {'id': 55, 'name': 'orange'}, 56: {'id': 56, 'name': 'broccoli'}, 57: {'id': 57, 'name': 'carrot'}, 58: {'id': 58, 'name': 'hot dog'}, 59: {'id': 59, 'name': 'pizza'}, 60: {'id': 60, 'name': 'donut'}, 61: {'id': 61, 'name': 'cake'}, 62: {'id': 62, 'name': 'chair'}, 63: {'id': 63, 'name': 'couch'}, 64: {'id': 64, 'name': 'potted plant'}, 65: {'id': 65, 'name': 'bed'}, 67: {'id': 67, 'name': 'dining table'}, 70: {'id': 70, 'name': 'toilet'}, 72: {'id': 72, 'name': 'tv'}, 73: {'id': 73, 'name': 'laptop'}, 74: {'id': 74, 'name': 'mouse'}, 75: {'id': 75, 'name': 'remote'}, 76: {'id': 76, 'name': 'keyboard'}, 77: {'id': 77, 'name': 'cell phone'}, 78: {'id': 78, 'name': 'microwave'}, 79: {'id': 79, 'name': 'oven'}, 80: {'id': 80, 'name': 'toaster'}, 81: {'id': 81, 'name': 'sink'}, 82: {'id': 82, 'name': 'refrigerator'}, 84: {'id': 84, 'name': 'book'}, 85: {'id': 85, 'name': 'clock'}, 86: {'id': 86, 'name': 'vase'}, 87: {'id': 87, 'name': 'scissors'}, 88: {'id': 88, 'name': 'teddy bear'}, 89: {'id': 89, 'name': 'hair drier'}, 90: {'id': 90, 'name': 'toothbrush'}}\n"
     ]
    }
   ],
   "source": [
    "print(category_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "executionInfo": {
     "elapsed": 55279,
     "status": "ok",
     "timestamp": 1603009651937,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "ss9LviHCVfhQ"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "executionInfo": {
     "elapsed": 55278,
     "status": "ok",
     "timestamp": 1603009651939,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "m3cvJL9jxOJH"
   },
   "outputs": [],
   "source": [
    "# A Python3 program to find if 2 given line segments intersect or not \n",
    "  \n",
    "class Point: \n",
    "    def __init__(self, x, y): \n",
    "        self.x = x \n",
    "        self.y = y \n",
    "\n",
    "# Given three colinear points p, q, r, the function checks if  \n",
    "# point q lies on line segment 'pr'  \n",
    "def onSegment(p, q, r): \n",
    "    if ( (q.x <= max(p.x, r.x)) and (q.x >= min(p.x, r.x)) and \n",
    "           (q.y <= max(p.y, r.y)) and (q.y >= min(p.y, r.y))): \n",
    "        return True\n",
    "    return False\n",
    "  \n",
    "def orientation(p, q, r): \n",
    "    # to find the orientation of an ordered triplet (p,q,r) \n",
    "    # function returns the following values: \n",
    "    # 0 : Colinear points \n",
    "    # 1 : Clockwise points \n",
    "    # 2 : Counterclockwise \n",
    "      \n",
    "    # See https://www.geeksforgeeks.org/orientation-3-ordered-points/amp/  \n",
    "    # for details of below formula.  \n",
    "      \n",
    "    val = (float(q.y - p.y) * (r.x - q.x)) - (float(q.x - p.x) * (r.y - q.y)) \n",
    "    if (val > 0): \n",
    "          \n",
    "        # Clockwise orientation \n",
    "        return 1\n",
    "    elif (val < 0): \n",
    "          \n",
    "        # Counterclockwise orientation \n",
    "        return 2\n",
    "    else: \n",
    "          \n",
    "        # Colinear orientation \n",
    "        return 0\n",
    "\n",
    "# The main function that returns true if  \n",
    "# the line segment 'p1q1' and 'p2q2' intersect. \n",
    "def doIntersect(p1,q1,p2,q2): \n",
    "      \n",
    "    # Find the 4 orientations required for  \n",
    "    # the general and special cases \n",
    "    o1 = orientation(p1, q1, p2) \n",
    "    o2 = orientation(p1, q1, q2) \n",
    "    o3 = orientation(p2, q2, p1) \n",
    "    o4 = orientation(p2, q2, q1) \n",
    "  \n",
    "    # General case \n",
    "    if ((o1 != o2) and (o3 != o4)): \n",
    "        return True\n",
    "  \n",
    "    # Special Cases \n",
    "  \n",
    "    # p1 , q1 and p2 are colinear and p2 lies on segment p1q1 \n",
    "    if ((o1 == 0) and onSegment(p1, p2, q1)): \n",
    "        return True\n",
    "  \n",
    "    # p1 , q1 and q2 are colinear and q2 lies on segment p1q1 \n",
    "    if ((o2 == 0) and onSegment(p1, q2, q1)): \n",
    "        return True\n",
    "  \n",
    "    # p2 , q2 and p1 are colinear and p1 lies on segment p2q2 \n",
    "    if ((o3 == 0) and onSegment(p2, p1, q2)): \n",
    "        return True\n",
    "  \n",
    "    # p2 , q2 and q1 are colinear and q1 lies on segment p2q2 \n",
    "    if ((o4 == 0) and onSegment(p2, q1, q2)): \n",
    "        return True\n",
    "  \n",
    "    # If none of the cases \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "executionInfo": {
     "elapsed": 55275,
     "status": "ok",
     "timestamp": 1603009651940,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "2_K8b7xVXmta"
   },
   "outputs": [],
   "source": [
    "def draw_centroid(frame, centroid):\n",
    "    red = (0, 0, 255)\n",
    "    value = tuple(centroid)\n",
    "    cv2.circle(frame, value, 1, red, 10)\n",
    "\n",
    "def draw_text(frame, centroid, id):\n",
    "    centroid = tuple(centroid)\n",
    "    green = (0, 255, 0)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    cv2.putText(frame, \"Person #\" + str(id), centroid, font, 2, green, 2, cv2.LINE_AA)\n",
    "\n",
    "def draw_counter_text(frame, count):\n",
    "    position = (100,100)\n",
    "    green = (0, 0, 255)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    cv2.putText(frame, \"Counter :\" + str(count), position, font, 3, green, 3, cv2.LINE_AA)\n",
    "\n",
    "def draw_in_out_text(frame, count_in, count_out):\n",
    "    position_in = (100,90)\n",
    "    position_out = (100,130)\n",
    "    green = (0, 0, 255)\n",
    "    font = cv2.FONT_HERSHEY_PLAIN\n",
    "    text_in = \"In : \" + str(count_in)\n",
    "    text_out = \"Out : \" + str(count_out)\n",
    "    cv2.putText(frame, text_in, position_in, font, 3, green, 3, cv2.LINE_AA)\n",
    "    cv2.putText(frame, text_out, position_out, font, 3, green, 3, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "executionInfo": {
     "elapsed": 967,
     "status": "ok",
     "timestamp": 1603012013371,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "h7qLV6khV9Yl"
   },
   "outputs": [],
   "source": [
    "# The following has been modified to fit TF2's vis util box normalized bounding box\n",
    "# import the necessary packages\n",
    "from scipy.spatial import distance as dist\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "class CentroidTracker():\n",
    "    def __init__(self, frame_shape, maxDisappeared=50, dualCounter=True):\n",
    "        # initialize the next unique object ID along with two ordered\n",
    "        # dictionaries used to keep track of mapping a given object\n",
    "        # ID to its centroid and number of consecutive frames it has\n",
    "        # been marked as \"disappeared\", respectively\n",
    "        self.nextObjectID = 0\n",
    "        self.objects = OrderedDict()\n",
    "        self.disappeared = OrderedDict()\n",
    "        # Height, width and color channel of the frame\n",
    "        self.height, self.width, self.channel = frame_shape\n",
    "        self.status = [0,0]\n",
    "        self.count_in = 0  # no. of visitors entering the building\n",
    "        self.count_out = 0 # no. of visitors exiting the building\n",
    "\n",
    "        # store the number of maximum consecutive frames a given\n",
    "        # object is allowed to be marked as \"disappeared\" until we\n",
    "        # need to deregister the object from tracking\n",
    "        self.maxDisappeared = maxDisappeared\n",
    "\n",
    "        # Display people enterring and exitting separately if set to true\n",
    "        self.dualCounter = dualCounter\n",
    "\n",
    "    def register(self, centroid):\n",
    "        # when registering an object we use the next available object\n",
    "        # ID to store the centroid\n",
    "        self.objects[self.nextObjectID] = centroid\n",
    "        self.disappeared[self.nextObjectID] = 0\n",
    "        self.nextObjectID += 1\n",
    "\n",
    "    def deregister(self, objectID):\n",
    "        # to deregister an object ID we delete the object ID from\n",
    "        # both of our respective dictionaries\n",
    "        del self.objects[objectID]\n",
    "        del self.disappeared[objectID]\n",
    "\n",
    "    def update(self, frame, rects, count=False, upper_ROI_start_point=None, upper_ROI_end_point=None, lower_ROI_start_point=None, lower_ROI_end_point=None):\n",
    "\n",
    "        # If counter displays people enterring and exitting the building \n",
    "        # separately\n",
    "        if self.dualCounter:\n",
    "            draw_in_out_text(frame, self.count_in, self.count_out) # draw in/out counter\n",
    "        # else if the counter only displays the number of people in the building\n",
    "        else:\n",
    "            counter = self.count_in - self.count_out\n",
    "            draw_counter_text(frame,counter)  # draw a counter\n",
    "\n",
    "        # check to see if the list of input bounding box rectangles\n",
    "        # is empty\n",
    "        if len(rects) == 0:\n",
    "            # loop over any existing tracked objects and mark them\n",
    "            # as disappeared\n",
    "            for objectID in list(self.disappeared.keys()):\n",
    "                self.disappeared[objectID] += 1\n",
    "\n",
    "                # if we have reached a maximum number of consecutive\n",
    "                # frames where a given object has been marked as\n",
    "                # missing, deregister it\n",
    "                if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                    self.deregister(objectID)\n",
    "\n",
    "            # return early as there are no centroids or tracking info\n",
    "            # to update\n",
    "            return self.objects\n",
    "\n",
    "        # initialize an array of input centroids for the current frame\n",
    "        inputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n",
    "\n",
    "        # loop over the bounding box rectangles\n",
    "        try :\n",
    "            for (i, (endY, endX, startY, startX)) in enumerate(rects):\n",
    "                # denormalize the bounding box coordinate\n",
    "                d_minX = startX * self.height\n",
    "                d_maxX = endX * self.height\n",
    "                d_minY = startY * self.width\n",
    "                d_maxY = endY * self.width\n",
    "                # use the bounding box coordinates to derive the centroid\n",
    "                cX = int((d_minX + d_maxX) / 2.0)\n",
    "                cY = int((d_minY + d_maxY) / 2.0)\n",
    "                inputCentroids[i] = (cX, cY)\n",
    "        except :\n",
    "            None\n",
    "\n",
    "        # if we are currently not tracking any objects take the input\n",
    "        # centroids and register each of them\n",
    "        if len(self.objects) == 0:\n",
    "            for i in range(0, len(inputCentroids)):\n",
    "                self.register(inputCentroids[i])\n",
    "                draw_centroid(frame, inputCentroids[i])  # draw centroid if new person is detected AND Tracker is not tracking\n",
    "                draw_text(frame, inputCentroids[i], self.nextObjectID-1)\n",
    "\n",
    "        # otherwise, are are currently tracking objects so we need to\n",
    "        # try to match the input centroids to existing object\n",
    "        # centroids\n",
    "        else:\n",
    "            # grab the set of object IDs and corresponding centroids\n",
    "            objectIDs = list(self.objects.keys())\n",
    "            objectCentroids = list(self.objects.values())\n",
    "\n",
    "            # compute the distance between each pair of object\n",
    "            # centroids and input centroids, respectively -- our\n",
    "            # goal will be to match an input centroid to an existing\n",
    "            # object centroid\n",
    "            D = dist.cdist(np.array(objectCentroids), inputCentroids)\n",
    "\n",
    "            # in order to perform this matching we must (1) find the\n",
    "            # smallest value in each row and then (2) sort the row\n",
    "            # indexes based on their minimum values so that the row\n",
    "            # with the smallest value as at the *front* of the index\n",
    "            # list\n",
    "            rows = D.min(axis=1).argsort()\n",
    "\n",
    "            # next, we perform a similar process on the columns by\n",
    "            # finding the smallest value in each column and then\n",
    "            # sorting using the previously computed row index list\n",
    "            cols = D.argmin(axis=1)[rows]\n",
    "\n",
    "            # in order to determine if we need to update, register,\n",
    "            # or deregister an object we need to keep track of which\n",
    "            # of the rows and column indexes we have already examined\n",
    "            usedRows = set()\n",
    "            usedCols = set()\n",
    "\n",
    "            # loop over the combination of the (row, column) index\n",
    "            # tuples\n",
    "            for (row, col) in zip(rows, cols):\n",
    "                # if we have already examined either the row or\n",
    "                # column value before, ignore it\n",
    "                # val\n",
    "                if row in usedRows or col in usedCols:\n",
    "                    continue\n",
    "\n",
    "                # otherwise, grab the object ID for the current row,\n",
    "                # set its new centroid, and reset the disappeared\n",
    "                # counter\n",
    "                objectID = objectIDs[row]\n",
    "\n",
    "                # Check to see first if that object has crossed the ROI\n",
    "                if count:  # counting only starts if it's enabled\n",
    "                    if (self.objects[objectID]).all():\n",
    "                        beforeMove = Point(int(self.objects[objectID][0]), int(self.objects[objectID][1]))\n",
    "                        afterMove = Point(int(inputCentroids[col][0]), int(inputCentroids[col][1]))\n",
    "\n",
    "                        if doIntersect(upper_ROI_start_point, upper_ROI_end_point, beforeMove, afterMove): #check upper detection\n",
    "                            self.status[0] = 1 #set to 1 as the person has crossed upper line\n",
    "\n",
    "                        if doIntersect(lower_ROI_start_point, lower_ROI_end_point, beforeMove, afterMove): #check lower detection\n",
    "                            self.status[1] = 1 #set to 1 as the person has lower upper line\n",
    "\n",
    "                        if self.status == [1,1]: #if that person has crossed both lines, reset status to 0 and now able to count up\n",
    "                            self.status = [0,0]\n",
    "                            # If the current centroid is below the ROI, counter goes up\n",
    "                            # meaning an object has entered the building\n",
    "                            if afterMove.y > lower_ROI_start_point.y:  # if y is higher means it's below in the actual frame\n",
    "                                self.count_in += 1\n",
    "                            else:\n",
    "                                self.count_out -= 1  # if it's the opposite means an object has exited the building\n",
    "\n",
    "                self.objects[objectID] = inputCentroids[col]\n",
    "                self.disappeared[objectID] = 0\n",
    "                draw_centroid(frame, inputCentroids[col])  # draw centroid when a person moved\n",
    "                draw_text(frame, inputCentroids[col], objectID)\n",
    "\n",
    "                # indicate that we have examined each of the row and\n",
    "                # column indexes, respectively\n",
    "                usedRows.add(row)\n",
    "                usedCols.add(col)\n",
    "\n",
    "            # compute both the row and column index we have NOT yet\n",
    "            # examined\n",
    "            unusedRows = set(range(0, D.shape[0])).difference(usedRows)\n",
    "            unusedCols = set(range(0, D.shape[1])).difference(usedCols)\n",
    "\n",
    "            # in the event that the number of object centroids is\n",
    "            # equal or greater than the number of input centroids\n",
    "            # we need to check and see if some of these objects have\n",
    "            # potentially disappeared\n",
    "            if D.shape[0] >= D.shape[1]:\n",
    "                # loop over the unused row indexes\n",
    "                for row in unusedRows:\n",
    "                    # grab the object ID for the corresponding row\n",
    "                    # index and increment the disappeared counter\n",
    "                    objectID = objectIDs[row]\n",
    "                    self.disappeared[objectID] += 1\n",
    "\n",
    "                    # check to see if the number of consecutive\n",
    "                    # frames the object has been marked \"disappeared\"\n",
    "                    # for warrants deregistering the object\n",
    "                    if self.disappeared[objectID] > self.maxDisappeared:\n",
    "                        self.deregister(objectID)\n",
    "\n",
    "            # otherwise, if the number of input centroids is greater\n",
    "            # than the number of existing object centroids we need to\n",
    "            # register each new input centroid as a trackable object\n",
    "            else:\n",
    "                for col in unusedCols:\n",
    "                    self.register(inputCentroids[col])\n",
    "                    draw_centroid(frame, inputCentroids[col])  # draw a centroid when a new person is detected\n",
    "                    draw_text(frame, inputCentroids[col], self.nextObjectID-1)\n",
    "\n",
    "        # return the set of trackable objects\n",
    "        return self.objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "executionInfo": {
     "elapsed": 54130,
     "status": "ok",
     "timestamp": 1603009651941,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "gaFHjoofWEdH"
   },
   "outputs": [],
   "source": [
    "def confidence_pruning(boxes, scores, min_threshold=0.3):\n",
    "    indices = np.squeeze(np.argwhere(scores >= min_threshold))\n",
    "    boxes_pruned = boxes[[indices]]\n",
    "    if boxes_pruned.ndim < 2:\n",
    "        boxes_pruned = np.expand_dims(boxes_pruned, axis=0)\n",
    "    return boxes_pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "executionInfo": {
     "elapsed": 28767,
     "status": "ok",
     "timestamp": 1603009651942,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "Tzl6pFtNWGwg"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR VIDEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 258
    },
    "executionInfo": {
     "elapsed": 26784,
     "status": "ok",
     "timestamp": 1603012443834,
     "user": {
      "displayName": "Yesaya Setyo",
      "photoUrl": "",
      "userId": "06058237594517046811"
     },
     "user_tz": -420
    },
    "id": "OoEyW_x9VgjL",
    "outputId": "5279425f-61d0-494f-8bf6-39544a1c10e2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fps : 12\n",
      "res : (1920, 1080)\n",
      "255 s are skipped!\n",
      "Ready to process!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:91: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:93: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:95: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00233438 0.40487048 0.17842698 0.4467235 ]]\n",
      " progress: 0 s / 5 s [[0.00231365 0.4048425  0.17848444 0.44675136]]\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 0 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 1 s / 5 s []\n",
      " progress: 2 s / 5 s []\n",
      " progress: 2 s / 5 s []\n",
      " progress: 2 s / 5 s [[0.22678836 0.47201586 0.9839931  0.5712341 ]]\n",
      " progress: 2 s / 5 s [[0.22676083 0.47198504 0.98399246 0.5712753 ]]\n",
      " progress: 2 s / 5 s [[0.24914779 0.46841386 0.988426   0.5730791 ]]\n",
      " progress: 2 s / 5 s [[0.24761884 0.46822414 0.98904634 0.57332754]]\n",
      " progress: 2 s / 5 s [[0.269367   0.4653214  0.99446094 0.5749128 ]]\n",
      " progress: 2 s / 5 s [[0.2693572  0.46532106 0.9944901  0.57488763]]\n",
      " progress: 2 s / 5 s [[0.3005075  0.46943825 0.99250114 0.5811654 ]]\n",
      " progress: 2 s / 5 s [[0.30057088 0.46942258 0.99222726 0.5811775 ]]\n",
      " progress: 2 s / 5 s [[0.34787902 0.47397947 0.99717885 0.58519745]]\n",
      " progress: 2 s / 5 s [[0.34787124 0.4740026  0.9973214  0.58516526]]\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s [[0.5608602 0.5040092 1.        0.6308481]]\n",
      " progress: 3 s / 5 s [[0.5608715 0.5040008 1.        0.6307907]]\n",
      " progress: 3 s / 5 s []\n",
      " progress: 3 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 4 s / 5 s []\n",
      " progress: 5 s / 5 s \n",
      "DONE!\n",
      "Saved as : G:\\mmp\\outVid\\2020-10-205__th0.2_mN2_1sept_kopioey-12frame.mp4\n",
      "Finish in : 22.93441653251648 (s)\n"
     ]
    }
   ],
   "source": [
    "if MODEL_NAME == 'efficientdet_d2_coco17_tpu-32__1':\n",
    "    mName = '1'\n",
    "elif MODEL_NAME == 'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8':\n",
    "    mName = '2'\n",
    "else :\n",
    "    mName = '0'\n",
    "today = str(date.today())\n",
    "\n",
    "thresh = 0.2 #threshold for filtering person\n",
    "\n",
    "### INPUT & OUTPUT VIDEO FILES ###\n",
    "#25 AGST 20 1145 1200.avi #my main vid\n",
    "#1 sept 2020 1100 1130.avi #second vid\n",
    "VIDEO_NAME = '1sept_kopioey-12frame.mp4'\n",
    "VIDEODIR = 'G:\\\\mmp\\\\' + VIDEO_NAME\n",
    "OUTDIR = 'G:\\\\mmp\\\\outVid\\\\' + today + '5_' + '_th' + str(thresh) + '_mN' + mName + '_' + VIDEO_NAME\n",
    "\n",
    "### SETTING UP CV2 & VIDEO ###\n",
    "cap = cv2.VideoCapture(VIDEODIR) #Getting video for cv2\n",
    "#default centerDet thresh = 0.55\n",
    "#centerDet d2 = 0.45 1st run\n",
    "#centerDet d2 = 0.4  2nd run\n",
    "#default centerNet thresh = 0.3 ~ 0.35\n",
    "\n",
    "### VIDEO TECHNICAL ###\n",
    "ret, image_np = cap.read()\n",
    "fshape = image_np.shape\n",
    "fheight = fshape[0]\n",
    "fwidth = fshape[1]\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "print(\"fps :\",fps)\n",
    "res = (fwidth , fheight)\n",
    "print(\"res :\",res)\n",
    "#.mp4 format : *'MP4V' ; .avi : *'XVID'\n",
    "out = cv2.VideoWriter(OUTDIR,cv2.VideoWriter_fourcc(*'MP4V'), fps, res)\n",
    "\n",
    "### GETTING CENTROID IF ANY DETECTION ###\n",
    "Tracker = CentroidTracker([fwidth,fheight,3], maxDisappeared=30)\n",
    "\n",
    "### LINE INTERSECT FOR COUNTING\n",
    "##Upper ROI\n",
    "u_start_point = (0, int(fheight*0.67))\n",
    "u_end_point = (int(fwidth), int(fheight*0.67))\n",
    "color = (255,0,0)\n",
    "thickness = 3\n",
    "upper_ROI_start_point = Point(u_start_point[0], u_start_point[1])\n",
    "upper_ROI_end_point = Point(u_end_point[0], u_end_point[1])\n",
    "\n",
    "##Lower ROI\n",
    "l_start_point = (0, int(fheight*0.75))\n",
    "l_end_point = (int(fwidth), int(fheight*0.75))\n",
    "color = (255,0,0)\n",
    "thickness = 3\n",
    "lower_ROI_start_point = Point(l_start_point[0], l_start_point[1])\n",
    "lower_ROI_end_point = Point(l_end_point[0], l_end_point[1])\n",
    "\n",
    "### SETTING UP WHEN to START & STOP, SHOWING DURATION ###   <---------------------------------###########################\n",
    "startDetection = 4 * 60 + 15 #when to start detect\n",
    "length = 1 * 5 #in second(s)  #Length how long to detect\n",
    "\n",
    "l_startDetection = startDetection * fps #how many frames to skip detect\n",
    "cap.set(cv2.CAP_PROP_POS_FRAMES, l_startDetection) #skip frame\n",
    "l_fps = length * fps #how many frames to detect\n",
    "i = 0 #initiate first frame\n",
    "\n",
    "### IT'S ALL BEGIN ###\n",
    "if not (startDetection == 0):\n",
    "    print('{} s are skipped!'.format(startDetection))\n",
    "print('Ready to process!')\n",
    "\n",
    "label_id_offset = 1\n",
    "\n",
    "startTime = time.time() #how long it takes to process desired length video\n",
    "\n",
    "while (i < l_fps): #start to detect\n",
    "    # Read frame from video\n",
    "    ret, image_np = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "  #Draw ROI line for counting\n",
    "    cv2.line(image_np, u_start_point, u_end_point, color, thickness) #upper part\n",
    "    cv2.line(image_np, l_start_point, l_end_point, color, thickness) #lower part\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "    indices = np.squeeze(np.argwhere((detections['detection_classes'][0].numpy() + label_id_offset).astype(int) == 1))\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    boxes = boxes[[indices]]\n",
    "    classes = (detections['detection_classes'][0] + label_id_offset).numpy()\n",
    "    classes = classes[[indices]].astype(int)\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    scores = scores[[indices]]\n",
    "    boxes = confidence_pruning(boxes, scores, thresh)\n",
    "    print(boxes)\n",
    "\n",
    "    People = Tracker.update(image_np, boxes, count=True, upper_ROI_start_point=upper_ROI_start_point, upper_ROI_end_point=upper_ROI_end_point, lower_ROI_start_point=lower_ROI_start_point, lower_ROI_end_point=lower_ROI_end_point)\n",
    "\n",
    "    print(\"\\r progress: {} s / {} s\".format(str(int(((i+1)/fps))),length), end =\" \")\n",
    "\n",
    "    # Display output\n",
    "    #nFrame = cv2.resize(image_np_with_detections, (800, 600))\n",
    "    #cv2.imshow('object detection', nFrame)\n",
    "\n",
    "    # write the frame\n",
    "    out.write(image_np)\n",
    "\n",
    "    i = i+1\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "doneTime = time.time()-startTime\n",
    "print(\"\")\n",
    "print(\"DONE!\")\n",
    "print(\"Saved as :\",OUTDIR)\n",
    "print(\"Finish in : {} (s)\".format(float(doneTime)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOR WEBCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res : (640, 480)\n",
      "Ready to process!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:72: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:74: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:76: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "C:\\Users\\yeset\\Anaconda3\\envs\\tf_gpu\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15532732 0.22830194 1.         0.9356082 ]\n",
      " [0.1472851  0.01880223 1.         0.9736452 ]]\n",
      "fps 0.3231351375579834s\n",
      "[[0.15190797 0.24603415 1.         0.9332912 ]]\n",
      "fps 0.3106203079223633s\n",
      "[[0.15438907 0.24122903 1.         0.93886983]\n",
      " [0.14300208 0.0121921  1.         0.980656  ]]\n",
      "fps 0.2978687286376953s\n",
      "[[0.15144655 0.26641378 1.         0.9284843 ]]\n",
      "fps 0.30995798110961914s\n",
      "[[0.13821058 0.26133922 1.         0.94934773]]\n",
      "fps 0.30089879035949707s\n",
      "[[0.14524615 0.24417996 1.         0.78034604]\n",
      " [0.15163279 0.24202836 1.         0.937598  ]]\n",
      "fps 0.29573655128479004s\n",
      "[[0.14957163 0.21458262 1.         0.7774427 ]]\n",
      "fps 0.2923729419708252s\n",
      "[[0.15223674 0.17559901 1.         0.8166686 ]]\n",
      "fps 0.2975656986236572s\n",
      "[[0.14988002 0.20863295 1.         0.78457093]\n",
      " [0.15115523 0.22545242 1.         0.93915653]]\n",
      "fps 0.2993886470794678s\n",
      "[[0.16730492 0.22306502 1.         0.94127715]\n",
      " [0.14416182 0.0132432  1.         0.9807807 ]]\n",
      "fps 0.2974405288696289s\n",
      "[[0.14673468 0.14012757 1.         0.85289013]\n",
      " [0.16825008 0.2341888  1.         0.9454484 ]\n",
      " [0.15151398 0.1828905  1.         0.87194014]]\n",
      "fps 0.29626917839050293s\n",
      "[[0.16814551 0.16753095 1.         0.82565534]\n",
      " [0.17284171 0.2238732  1.         0.9401923 ]]\n",
      "fps 0.29720020294189453s\n",
      "[[0.2133042  0.24825022 1.         0.7593349 ]]\n",
      "fps 0.29860997200012207s\n",
      "[[0.23993978 0.24925771 0.9989959  0.7756218 ]]\n",
      "fps 0.31553149223327637s\n",
      "[]\n",
      "fps 0.2962327003479004s\n",
      "[]\n",
      "fps 0.2941141128540039s\n",
      "[]\n",
      "fps 0.29941797256469727s\n",
      "[]\n",
      "fps 0.2940521240234375s\n",
      "[]\n",
      "fps 0.3018343448638916s\n",
      "[]\n",
      "fps 0.3004028797149658s\n",
      "[]\n",
      "fps 0.29864048957824707s\n",
      "[]\n",
      "fps 0.29155850410461426s\n",
      "[]\n",
      "fps 0.2986409664154053s\n",
      "[]\n",
      "fps 0.30036330223083496s\n",
      "[]\n",
      "fps 0.2994215488433838s\n",
      "[[0.5541779  0.33751327 0.9999305  0.7501729 ]]\n",
      "fps 0.3050971031188965s\n",
      "[]\n",
      "fps 0.2954127788543701s\n",
      "[[0.50804037 0.32198986 1.         0.7317749 ]]\n",
      "fps 0.29312992095947266s\n",
      "[[0.40665287 0.2905385  1.         0.71679467]]\n",
      "fps 0.3020610809326172s\n",
      "[]\n",
      "fps 0.2986264228820801s\n",
      "[]\n",
      "fps 0.2961151599884033s\n",
      "[]\n",
      "fps 0.29761695861816406s\n",
      "[[0.21686554 0.01470965 1.         0.97885287]\n",
      " [0.19039087 0.04196942 1.         0.71557605]\n",
      " [0.21460173 0.01472461 1.         0.93095607]]\n",
      "fps 0.2946455478668213s\n",
      "[[0.2019454  0.02398339 0.9967681  0.9680753 ]]\n",
      "fps 0.30144500732421875s\n",
      "[[0.19635706 0.06772548 1.         0.78317815]\n",
      " [0.2138323  0.02175039 1.         0.97139144]]\n",
      "fps 0.30877161026000977s\n",
      "[[0.1999722  0.06701812 1.         0.7838429 ]\n",
      " [0.21231453 0.09782588 1.         0.80053616]\n",
      " [0.21105513 0.07327721 1.         0.91908157]]\n",
      "fps 0.29566335678100586s\n",
      "[[0.23101482 0.09270945 1.         0.8212763 ]\n",
      " [0.23014271 0.07079154 1.         0.92126215]]\n",
      "fps 0.3029327392578125s\n",
      "[[0.24231514 0.16008595 1.         0.7857293 ]]\n",
      "fps 0.29775118827819824s\n",
      "[]\n",
      "fps 0.3103492259979248s\n",
      "[]\n",
      "fps 0.29652929306030273s\n",
      "[]\n",
      "fps 0.298797607421875s\n",
      "[[0.33319783 0.28268313 1.         0.8185688 ]]\n",
      "fps 0.3018605709075928s\n",
      "[]\n",
      "fps 0.2950160503387451s\n",
      "[]\n",
      "fps 0.29850339889526367s\n",
      "[]\n",
      "fps 0.2977569103240967s\n",
      "[[0.21678393 0.06559321 1.         0.8009634 ]]\n",
      "fps 0.29629945755004883s\n",
      "[[0.17686197 0.03526768 1.         0.958032  ]\n",
      " [0.17638187 0.03760207 1.         0.8920957 ]\n",
      " [0.17694013 0.04842719 0.99961704 0.83396125]]\n",
      "fps 0.30127477645874023s\n",
      "[[0.1556066  0.02233085 0.99949646 0.971496  ]]\n",
      "fps 0.3124203681945801s\n",
      "[[0.1328722  0.03583232 1.         0.9559716 ]]\n",
      "fps 0.6961934566497803s\n",
      "[[0.13447289 0.02888784 0.99893045 0.93296087]]\n",
      "fps 0.8104860782623291s\n",
      "[[0.15303215 0.02377802 1.         0.9692486 ]]\n",
      "fps 0.40975451469421387s\n",
      "[]\n",
      "fps 0.4004387855529785s\n",
      "[]\n",
      "fps 0.40273261070251465s\n",
      "[]\n",
      "fps 0.41142892837524414s\n",
      "[]\n",
      "fps 0.40950894355773926s\n",
      "[]\n",
      "fps 0.4118666648864746s\n",
      "[]\n",
      "fps 0.40811800956726074s\n",
      "[]\n",
      "fps 0.40594029426574707s\n",
      "[]\n",
      "fps 0.4016883373260498s\n",
      "[[0.17649388 0.04907924 1.         0.8176326 ]]\n",
      "fps 0.4022214412689209s\n",
      "[[0.18375322 0.02812743 1.         0.9496992 ]]\n",
      "fps 0.40233683586120605s\n",
      "[[0.18728101 0.02980623 1.         0.9767716 ]]\n",
      "fps 0.41182494163513184s\n",
      "[]\n",
      "fps 0.40776968002319336s\n",
      "[[0.1531239  0.03317434 1.         0.9588232 ]]\n",
      "fps 0.40822529792785645s\n",
      "[[0.15028422 0.03069586 1.         0.9610681 ]]\n",
      "fps 0.40940046310424805s\n",
      "[[0.14788637 0.02501506 1.         0.9663604 ]]\n",
      "fps 0.4329211711883545s\n",
      "[]\n",
      "fps 0.5234074592590332s\n",
      "[]\n",
      "fps 0.5289053916931152s\n",
      "[[0.10689557 0.0079048  1.         0.9827908 ]]\n",
      "fps 0.5315523147583008s\n",
      "[]\n",
      "fps 0.5465390682220459s\n",
      "[[0.08800479 0.02545047 1.         0.96599203]\n",
      " [0.11108315 0.01236677 1.         0.91763115]]\n",
      "fps 0.5429844856262207s\n",
      "[[0.08633983 0.01279473 1.         0.9778837 ]]\n",
      "fps 0.5516922473907471s\n",
      "[[0.09066682 0.01565289 1.         0.97576886]\n",
      " [0.3916777  0.66792023 0.6627332  0.7785295 ]]\n",
      "fps 0.5364363193511963s\n",
      "[[0.08625464 0.00870889 1.         0.98188394]]\n",
      "fps 0.5251061916351318s\n",
      "[[0.0868119  0.00731623 1.         0.970638  ]]\n",
      "fps 0.5247697830200195s\n",
      "[[0.10465976 0.01605859 1.         0.9600731 ]]\n",
      "fps 0.5253853797912598s\n",
      "[[0.08768865 0.01354325 1.         0.97857666]]\n",
      "fps 0.5272862911224365s\n",
      "[[0.10571437 0.01077598 1.         0.9671961 ]]\n",
      "fps 0.5270669460296631s\n",
      "[[0.1021833  0.01475099 1.         0.975549  ]]\n",
      "fps 0.5314967632293701s\n",
      "[[0.10866706 0.00856388 1.         0.9218582 ]]\n",
      "fps 0.5279505252838135s\n",
      "[[0.08615708 0.01018444 1.         0.9675995 ]]\n",
      "fps 0.5252997875213623s\n",
      "\n",
      "DONE!\n",
      "Saved as : G:\\mmp\\outVid\\2020-10-205__th0.2_mN2_1sept_kopioey-12frame.mp4\n",
      "Finish in : 22.93441653251648 (s)\n"
     ]
    }
   ],
   "source": [
    "if MODEL_NAME == 'efficientdet_d2_coco17_tpu-32__1':\n",
    "    mName = '1'\n",
    "elif MODEL_NAME == 'centernet_resnet101_v1_fpn_512x512_coco17_tpu-8':\n",
    "    mName = '2'\n",
    "else :\n",
    "    mName = '0'\n",
    "today = str(date.today())\n",
    "\n",
    "thresh = 0.2 #threshold for filtering person\n",
    "\n",
    "### SETTING UP CV2 & VIDEO ###\n",
    "cap = cv2.VideoCapture(0) #Getting video for cv2\n",
    "#default centerDet thresh = 0.55\n",
    "#centerDet d2 = 0.45 1st run\n",
    "#centerDet d2 = 0.4  2nd run\n",
    "#default centerNet thresh = 0.3 ~ 0.35\n",
    "\n",
    "### VIDEO TECHNICAL ###\n",
    "ret, image_np = cap.read()\n",
    "fshape = image_np.shape\n",
    "fheight = fshape[0]\n",
    "fwidth = fshape[1]\n",
    "res = (fwidth , fheight)\n",
    "print(\"res :\",res)\n",
    "\n",
    "### GETTING CENTROID IF ANY DETECTION ###\n",
    "Tracker = CentroidTracker([fwidth,fheight,3], maxDisappeared=30)\n",
    "\n",
    "### LINE INTERSECT FOR COUNTING\n",
    "##Upper ROI\n",
    "u_start_point = (0, int(fheight*0.67))\n",
    "u_end_point = (int(fwidth), int(fheight*0.67))\n",
    "color = (255,0,0)\n",
    "thickness = 3\n",
    "upper_ROI_start_point = Point(u_start_point[0], u_start_point[1])\n",
    "upper_ROI_end_point = Point(u_end_point[0], u_end_point[1])\n",
    "\n",
    "##Lower ROI\n",
    "l_start_point = (0, int(fheight*0.75))\n",
    "l_end_point = (int(fwidth), int(fheight*0.75))\n",
    "color = (255,0,0)\n",
    "thickness = 3\n",
    "lower_ROI_start_point = Point(l_start_point[0], l_start_point[1])\n",
    "lower_ROI_end_point = Point(l_end_point[0], l_end_point[1])\n",
    "\n",
    "print('Ready to process!')\n",
    "\n",
    "label_id_offset = 1\n",
    "\n",
    "while True: #start to detect\n",
    "    startTime = time.time() #how long it takes to process desired length video\n",
    "    # Read frame from video\n",
    "    ret, image_np = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print('not detecting frame, quitting!')\n",
    "        break\n",
    "        \n",
    "    k = cv2.waitKey(1)\n",
    "    if k == 27:          ## lower this number goes faster frame freezes.\n",
    "        break        \n",
    "\n",
    "    #Draw ROI line for counting\n",
    "    cv2.line(image_np, u_start_point, u_end_point, color, thickness) #upper part\n",
    "    cv2.line(image_np, l_start_point, l_end_point, color, thickness) #lower part\n",
    "\n",
    "    input_tensor = tf.convert_to_tensor(np.expand_dims(image_np, 0), dtype=tf.float32)\n",
    "    detections, predictions_dict, shapes = detect_fn(input_tensor)\n",
    "\n",
    "    indices = np.squeeze(np.argwhere((detections['detection_classes'][0].numpy() + label_id_offset).astype(int) == 1))\n",
    "    boxes = detections['detection_boxes'][0].numpy()\n",
    "    boxes = boxes[[indices]]\n",
    "    classes = (detections['detection_classes'][0] + label_id_offset).numpy()\n",
    "    classes = classes[[indices]].astype(int)\n",
    "    scores = detections['detection_scores'][0].numpy()\n",
    "    scores = scores[[indices]]\n",
    "    boxes = confidence_pruning(boxes, scores, thresh)\n",
    "    print(boxes)\n",
    "\n",
    "    People = Tracker.update(image_np, boxes, count=True, upper_ROI_start_point=upper_ROI_start_point, upper_ROI_end_point=upper_ROI_end_point, lower_ROI_start_point=lower_ROI_start_point, lower_ROI_end_point=lower_ROI_end_point)\n",
    "\n",
    "    print(\"fps {}s\".format(str(time.time()-startTime), end =\" \"))\n",
    "\n",
    "    # Display output\n",
    "    #nFrame = cv2.resize(image_np_with_detections, (800, 600))\n",
    "    cv2.imshow('object_detection', image_np)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\")\n",
    "print(\"DONE!\")\n",
    "print(\"Saved as :\",OUTDIR)\n",
    "print(\"Finish in : {} (s)\".format(float(doneTime)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOt7btD796zwoFO6TkO9sj3",
   "name": "Obj Det_Alpha.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
